CKPTS:      # CUDA_VISIBLE_DEVICES=2 python main.py -c cfgs/cfgs_her2st.yaml 
  exp_code:         DeMM_phikon2_ls2_rk16                # experiment code
  data_root:        example_data/her2st           # data directory containing the raw data for each task
  split_data_root:  cross_val_splits/her2st       # directory containing the split files for each dataset
  split_ids:        [0, 1, 2, 3]                  # split ids used for training
  results_dir:      results                       # directory where results will be dumped
  independent_root:                               # directory containing the independent test data
  independent_list:                               # list of independent test sets

HyperParams:
  max_epochs: 100           # maximum number of epochs to train (default: 100)
  batch_size: 128           # input batch size for training (default: 128)
  num_workers: 16           # data loader workers (default: 16)
  lr_rate: 0.002            # learning rate (default: 0.002)
  optim_fn: adam            # optimizer [adam, sgd, rmsprop, adadelta, adagrad, adamw]
  weight_reg: 0.0001        # weight decay (default: 1e-4)
  scheduler_fn: OneCycleLR  # optimizer scheduler [CosineAnnealingLR CyclicLR LinearLR OneCycleLR StepLR, or none]

  loss_main:  RMSE          # balanceMSE # ['RMSE', 'MSE', 'CE', 'L1', 'Pearson'] ' loss function (default: RMSE)'
  loss_rec:   RMSE          # balanceMSE # ['RMSE', 'MSE', 'CE', 'L1', 'Pearson'] ' loss function (default: RMSE)'
  loss_align: RMSE          # balanceMSE # ['RMSE', 'MSE', 'CE', 'L1', InfoNCE] ' loss function (default: RMSE)'
  lambda_main:  0.3         # lambda weight for main pred loss (default: 0.3)
  lambda_rec:   0.6         # lambda weight for reconstruction loss (default: 0.6)
  lambda_align: 0.1        # lambda weight for cross-modal alignment loss (default: 0.1)

  pre_extracted: False      # use pre-extracted features or not (default: False)
  architecture: DeMM        # DeMM CUCA, CUCAMLP, hist2cell, FMMLP, MLP
  backbone:     phikon2     # virchow2, virchow  ...
  hidden_dim: 512           # hidden dimension for the model
  proj_dim:   512           # projection dimension for the model
  num_cls:    39            # number of classes for the model


LoraCfgParams:       
  ft_lora: True                   # whether to use lora or not
  r: 16                         # dimensions of a low-rank matrix
  lora_alpha: 16                  # alpha parameter in lora
  target_modules: []              # target modules for lora
  lora_dropout: 0.1               # dropout for lora
  bias: 'none'                    # bias for lora
  modules_to_save:                
  only_spec_blocks: ["22", "23"]  #  phikon2 uni_v1: ["22", "23"], virchow2: ["30", "31"], hoptimus0: ["38", "39"]


COMMON:
  gpu: '0'
  seed: 3407                      # 'random seed for reproducible experiment (default: 3407)'
